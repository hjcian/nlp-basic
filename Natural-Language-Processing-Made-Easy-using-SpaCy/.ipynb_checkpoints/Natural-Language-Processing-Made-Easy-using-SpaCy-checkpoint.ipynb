{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. SpaCy Pipeline and Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Tripadvisor_hotelreviews_Shivambansal.txt'\n",
    "document = open(filename, encoding='utf8').read()\n",
    "document = nlp(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在 document 是 spcay 的 english model，並且擁有許多 methods/properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '_bulk_merge',\n",
       " '_py_tokens',\n",
       " '_realloc',\n",
       " '_vector',\n",
       " '_vector_norm',\n",
       " 'cats',\n",
       " 'char_span',\n",
       " 'count_by',\n",
       " 'doc',\n",
       " 'ents',\n",
       " 'extend_tensor',\n",
       " 'from_array',\n",
       " 'from_bytes',\n",
       " 'from_disk',\n",
       " 'get_extension',\n",
       " 'get_lca_matrix',\n",
       " 'has_extension',\n",
       " 'has_vector',\n",
       " 'is_nered',\n",
       " 'is_parsed',\n",
       " 'is_sentenced',\n",
       " 'is_tagged',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'mem',\n",
       " 'merge',\n",
       " 'noun_chunks',\n",
       " 'noun_chunks_iterator',\n",
       " 'print_tree',\n",
       " 'remove_extension',\n",
       " 'retokenize',\n",
       " 'sentiment',\n",
       " 'sents',\n",
       " 'set_extension',\n",
       " 'similarity',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'to_array',\n",
       " 'to_bytes',\n",
       " 'to_disk',\n",
       " 'to_json',\n",
       " 'user_data',\n",
       " 'user_hooks',\n",
       " 'user_span_hooks',\n",
       " 'user_token_hooks',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此時來個 [Overview](https://spacy.io/api) 可能會讓你更好理解：\n",
    "- 中心物件為 `Doc` 與 `Vocab`\n",
    "    - `Doc` 儲存所有 tokens\n",
    "    - `Vocab` 為所有可得資訊的 **lookup table**\n",
    "- 一開始使用 `load()` 會選擇語言模型，接著使用 `nlp(texts)` 就幫你完成 `Pipeline` 及 `Tokenize` 的步驟拿到 Doc 物件\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Tokenization\n",
    "\n",
    "spacy 的 model 已經將文本分割成句子、而每一句皆已 tokenization 了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice\n",
      "boston\n"
     ]
    }
   ],
   "source": [
    "# first token of the doc\n",
    "print(document[0])\n",
    "\n",
    "# last token of the doc\n",
    "print(document[len(document)-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Nice place Better than some reviews give it credit for.,\n",
       " Overall, the rooms were a bit small but nice.,\n",
       " Everything was clean, the view was wonderful and it is very well located (the Prudential Center makes shopping and eating easy and the T is nearby for jaunts out and about the city).,\n",
       " Overall, it was a good experience and the staff was quite friendly. ,\n",
       " what a surprise]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of sentences of our doc\n",
    "list(document.sents)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{84: 'ADJ',\n",
       " 92: 'NOUN',\n",
       " 86: 'ADV',\n",
       " 85: 'ADP',\n",
       " 90: 'DET',\n",
       " 100: 'VERB',\n",
       " 95: 'PRON',\n",
       " 97: 'PUNCT',\n",
       " 89: 'CCONJ',\n",
       " 96: 'PROPN',\n",
       " 94: 'PART',\n",
       " 103: 'SPACE',\n",
       " 99: 'SYM',\n",
       " 93: 'NUM',\n",
       " 101: 'X',\n",
       " 87: 'AUX',\n",
       " 91: 'INTJ'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all tags\n",
    "all_tags = {w.pos: w.pos_ for w in document}\n",
    "all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice , Tag:  JJ\n",
      "place , Tag:  NN\n",
      "Better , Tag:  RBR\n",
      "than , Tag:  IN\n",
      "some , Tag:  DT\n",
      "reviews , Tag:  NNS\n",
      "give , Tag:  VBP\n",
      "it , Tag:  PRP\n",
      "credit , Tag:  NN\n",
      "for , Tag:  IN\n",
      ". , Tag:  .\n"
     ]
    }
   ],
   "source": [
    "# all tags of first sentence of our document\n",
    "for word in list(document.sents)[0]:\n",
    "    print(word, \", Tag: \", word.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 其中：\n",
    "\n",
    "|NAME|TYPE|DESCRIPTION|\n",
    "|---|---|---|\n",
    "|pos|int|Coarse-grained part-of-speech.|\n",
    "|pos_|unicode|Coarse-grained part-of-speech.|\n",
    "|tag|int|Fine-grained part-of-speech.|\n",
    "|tag_|unicode|Fine-grained part-of-speech.|\n",
    "\n",
    "- *粗粒度*\n",
    "- *細粒度*\n",
    "\n",
    "- 該 word 為 ***Token物件***，還有哪些屬性可參考官方文件：[Token物件 #attributes](https://spacy.io/api/token#attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hotel', 685),\n",
       " ('room', 653),\n",
       " ('great', 300),\n",
       " ('sheraton', 286),\n",
       " ('location', 272)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define some parameters  \n",
    "noisy_pos_tags = set([\"PROP\"])\n",
    "min_token_length = 2\n",
    "\n",
    "#Function to check if the token is a noise or not  \n",
    "def isNoise(token):     \n",
    "    if (token.pos_ in noisy_pos_tags) or \\\n",
    "       (token.is_stop == True) or \\\n",
    "       (len(token) <= min_token_length):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def cleanup(token, lower = True):\n",
    "    if lower:\n",
    "        return token.lower().strip()\n",
    "    return token.strip()\n",
    "\n",
    "# top unigrams used in the reviews \n",
    "from collections import Counter\n",
    "cleaned_list = [cleanup(word.text) for word in document if not isNoise(word)]\n",
    "Counter(cleaned_list).most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Entity Detection\n",
    "\n",
    "- Entities can be of different types, such as – ***person***, ***location***, ***organization***, ***dates***, ***numerals***, etc. \n",
    "- These entities can be accessed through ***“.ents”*** property.\n",
    "- [All Entities](https://spacy.io/api/annotation#named-entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CARDINAL',\n",
       " 'DATE',\n",
       " 'EVENT',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LOC',\n",
       " 'MONEY',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = set([w.label_ for w in document.ents]) \n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONEY\n",
      "     ['99', 'more than $80', '129', '18', 'about $135']\n",
      "ORDINAL\n",
      "     ['25th', '24th', '2nd', '5th', '29th']\n",
      "PRODUCT\n",
      "     ['the Rodeo Dr', 'The Atlantis Fitness', 'Copley', 'Highly', 'the USS Constitution']\n",
      "ORG\n",
      "     ['Quincy Market', 'Suite', 'PRICELINE', 'the Pour House', 'the Fitz Inn']\n",
      "NORP\n",
      "     ['Italian', 'T.', '65F.We', 'European', 'Hynes']\n",
      "PERSON\n",
      "     ['Keeps Me Coming Back', 'Sweet Sleeper - so', 'Duck', 'the Duck Tour', 'Taylor']\n",
      "EVENT\n",
      "     ['New Years', 'the Marathon Expo', 'the Hynes Convention Ctr', 'an International Seafood Show', 'Key Club International Convention']\n",
      "GPE\n",
      "     ['Hotwire', 'Backbay', 'Priceline', 'Sak鈥檚', 'the Ramada Hong Kong']\n",
      "FAC\n",
      "     ['22nd Floor', 'the Prudential Tower', 'Faneiul Hall', 'South Tower', 'the South Tower']\n",
      "CARDINAL\n",
      "     ['4-star', '9/6', 'about 400', '3/3', '1200']\n",
      "TIME\n",
      "     ['more than one night', 'the third night', 'the next morning', 'about an hour', '179CDN$/per night']\n",
      "LOC\n",
      "     ['Breakfast', 'the North Wing', 'Boston Harbor', 'Newbury St.', 'Back Bay']\n",
      "WORK_OF_ART\n",
      "     ['a USA Today', 'Beautiful Tree lined Streets', 'Great Location Spent', 'a King Room', 'Great Location Overall']\n",
      "LANGUAGE\n",
      "     ['English']\n",
      "PERCENT\n",
      "     ['100%', '50%', '20%']\n",
      "DATE\n",
      "     ['a day', 'monthly', 'over a year and one week', '6 years', '5 day']\n",
      "QUANTITY\n",
      "     ['100 feet', '6.30 am', 'three feet', 'a ton', '3 miles']\n"
     ]
    }
   ],
   "source": [
    "for label in labels: \n",
    "    entities = [ cleanup(e.text, lower=False) for e in document.ents if label==e.label_] \n",
    "    entities = list(set(entities)) \n",
    "    print(label)\n",
    "    print(\"    \", entities[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 其中，[`document.ents`](https://spacy.io/api/doc#ents)回傳的是 [Span物件](https://spacy.io/api/span)\n",
    "\n",
    "---\n",
    "\n",
    "## 2.4 Dependency Parsing\n",
    "- What is Dependency Parsing?\n",
    "    - [Dependency Parsing in NLP](https://shirishkadam.com/2016/12/23/dependency-parsing-in-nlp/)\n",
    "- 可用來幫助我們做語意分析，理解主詞、受詞等到底是哪個 token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[The hotel as stated is in a fantastic location and the Wrentham Village outlet is well worth a visit for bargain shopping ( the bus picks up outside).,\n",
       " The hotel bar is a little pricey ( not helped by the current dollar rate) but is a nice place to relax after a busy day shopping.,\n",
       " A cab from the airport to the hotel can be cheaper than the shuttles depending what time of the day you go.,\n",
       " Boston from 17th Floor of Sheraton Hotel \n",
       " Find an alternative to the Sheraton,\n",
       " We stayed at this hotel for 3 nights.]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract all review sentences that contains the term - hotel\n",
    "hotel = [sent for sent in document.sents if 'hotel' in sent.text.lower()]\n",
    "hotel[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We :  []\n",
      "stayed :  [We, at, for, .]\n",
      "at :  [hotel]\n",
      "this :  []\n",
      "hotel :  [this]\n",
      "for :  [nights]\n",
      "3 :  []\n",
      "nights :  [3]\n",
      ". :  []\n"
     ]
    }
   ],
   "source": [
    "sentence = hotel[4] \n",
    "for word in sentence:\n",
    "    print(word, ': ', str(list(word.children)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We stayed at this hotel for 3 nights.\n",
      "               stayed(VBD)(ROOT)                                  \n",
      "       ________________|_________________________________          \n",
      "      |                |           at(IN)(prep)    for(IN)(prep)  \n",
      "      |                |                |                |         \n",
      "      |                |         hotel(NN)(pobj) nights(NNS)(pobj)\n",
      "      |                |                |                |         \n",
      "We(PRP)(nsubj)    .(.)(punct)     this(DT)(det)    3(CD)(nummod)  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import Tree\n",
    "\n",
    "def tok_format(tok):\n",
    "    return '{}({})({})'.format(tok.orth_, tok.tag_, tok.dep_)\n",
    "#     return \"_\".join([tok.orth_, tok.tag_, tok.dep_])\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(tok_format(node), [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return tok_format(node)\n",
    "\n",
    "print(sentence)\n",
    "to_nltk_tree(sentence.root).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('other', 20),\n",
       " ('great', 10),\n",
       " ('nice', 7),\n",
       " ('good', 7),\n",
       " ('better', 6),\n",
       " ('Nice', 5),\n",
       " ('different', 5),\n",
       " ('many', 5),\n",
       " ('best', 4),\n",
       " ('wonderful', 3)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check all adjectives used with a word \n",
    "def pos_words (doc, token, ptag):\n",
    "    sentences = [sent for sent in doc.sents if token in sent.text]     \n",
    "    pwrds = []\n",
    "    for sent in sentences:\n",
    "        for word in sent:\n",
    "            if token in word.text: \n",
    "                pwrds.extend([child.text.strip() for child in word.children\n",
    "                                                      if child.pos_ == ptag] )\n",
    "    return Counter(pwrds).most_common(10)\n",
    "\n",
    "pos_words(document, 'hotel', \"ADJ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Word to Vectors Integration\n",
    "\n",
    "- [官方參考文件](https://spacy.io/usage/vectors-similarity)\n",
    "- [All Available pre-trained statistical models for English](https://spacy.io/models/en)\n",
    "- 英文模型就有四種，有甚麼不同呢？\n",
    "    - size 不同，越大越精細 (sm < md < lg)\n",
    "    - md 與 lg 才是 GloVe vectors\n",
    "\n",
    "***Use built-in GloVe word embeddings***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True 7.0336733 False\n",
      "cat True 6.6808186 False\n",
      "banana True 6.700014 False\n",
      "egawhgawh False 0.0 True\n",
      ". True 4.9316354 False\n"
     ]
    }
   ],
   "source": [
    "# download en_core_web_md first (~95.4 MB)\n",
    "# !python -m spacy download en_core_web_md\n",
    "import en_core_web_md\n",
    "\n",
    "# nlp_haha = spacy.load('en_core_web_md') \n",
    "# not works for me (Anaconda on Win10)\n",
    "\n",
    "nlp_haha = en_core_web_md.load() # works fine\n",
    "tokens = nlp_haha('dog cat banana egawhgawh.')\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog (True)\n",
      "   └╴ cat 0.80168545\n",
      "   └╴ banana 0.24327643\n",
      "   └╴ . 0.27271757\n",
      "cat (True)\n",
      "   └╴ banana 0.28154364\n",
      "   └╴ . 0.28120673\n",
      "banana (True)\n",
      "   └╴ . 0.18991143\n",
      "egawhgawh (False)\n"
     ]
    }
   ],
   "source": [
    "for i, i_token in enumerate(tokens[:-1]):\n",
    "    print(\"{} ({})\".format(i_token, i_token.has_vector))\n",
    "    for j, j_token in enumerate(tokens[i + 1:]):\n",
    "        if i_token.has_vector and j_token.has_vector:\n",
    "            print(\"   └╴\", j_token, i_token.similarity(j_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "print(tokens[0].vector.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Use custom word vectors*** ([官方指引](https://spacy.io/usage/vectors-similarity#custom))\n",
    "- 大部分的 word vectors 格式皆會輸出成一行一行、每行為 word <vecotr...> 的格式\n",
    "- 在給 spacy 讀取之前需先用 [cli tool](https://spacy.io/api/cli#init-model) 將純文字檔轉成 bianry format，確切的格式須遵守 Word2Vec format\n",
    "    - on **linux**: `python -m spacy init-model en custom_vectors --vectors-loc custom_vectors.txt`\n",
    "    - on **windows**: `python.exe -m spacy init-model en custom_vectors --vectors-loc custom_vectors.txt`\n",
    "    - ![init-model.png](./img/init-model.png)\n",
    "- Word2Vec format 例子詳見 [custom_vectors.txt](./custom_vectors.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1: \n",
      "haha True 0.7416199 False\n",
      "uccu False 0.0 True\n",
      ". False 0.0 True\n",
      "\n",
      " Doc 2: \n",
      "hello False 0.0 True\n",
      "cellopoint True 0.7416199 False\n",
      ". False 0.0 True\n",
      "\n",
      " 驗算 Norm:  0.7416198487095663\n"
     ]
    }
   ],
   "source": [
    "nlp_haha = spacy.load(\"./custom_vectors\") # assign directory\n",
    "doc1 = nlp_haha(\"haha uccu.\")\n",
    "doc2 = nlp_haha(\"hello cellopoint.\")\n",
    "\n",
    "print(\"Doc 1: \")\n",
    "for token in doc1:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)\n",
    "    \n",
    "print(\"\\n\", \"Doc 2: \")\n",
    "for token in doc2:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)\n",
    "\n",
    "print(\"\\n\", \"驗算 Norm: \", (0.1**2 + 0.2**2 + 0.3**2 + 0.4**2 + 0.5**2)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000000028574667\n"
     ]
    }
   ],
   "source": [
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Machine Learning with text using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.base import TransformerMixin \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "\n",
    "import en_core_web_md\n",
    "parser = en_core_web_md.load()\n",
    "\n",
    "#Custom transformer using spaCy \n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [clean_text(text) for text in X]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic utility function to clean the text \n",
    "def clean_text(text):     \n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create spacy tokenizer that parses a sentence and generates tokens\n",
    "#these can also be replaced by word vectors \n",
    "def spacy_tokenizer(sentence):\n",
    "    tokens = parser(sentence)\n",
    "    tokens = [tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_ for tok in tokens]\n",
    "    tokens = [tok for tok in tokens if (tok not in stopwords and tok not in punctuations)]\n",
    "    return tokens\n",
    "\n",
    "#create vectorizer object to generate feature vectors, we will use custom spacy’s tokenizer\n",
    "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1,1))\n",
    "classifier = LinearSVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 其中：`-PRON-` 是 SpaCy 處理**人稱代詞**的詞形還原實作選擇，給個特殊符號代替。原文請參考[api/annotation#lemmatization](https://spacy.io/api/annotation#lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the  pipeline to clean, tokenize, vectorize, and classify \n",
    "pipe = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', vectorizer),\n",
    "                 ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "train = [('I love this sandwich.', 'pos'),          \n",
    "         ('this is an amazing place!', 'pos'),\n",
    "         ('I feel very good about these beers.', 'pos'),\n",
    "         ('this is my best work.', 'pos'),\n",
    "         (\"what an awesome view\", 'pos'),\n",
    "         ('I do not like this restaurant', 'neg'),\n",
    "         ('I am tired of this stuff.', 'neg'),\n",
    "         (\"I can't deal with this\", 'neg'),\n",
    "         ('he is my sworn enemy!', 'neg'),          \n",
    "         ('my boss is horrible.', 'neg')]\n",
    "\n",
    "test =   [('the beer was good.', 'pos'),     \n",
    "         ('I do not enjoy my job', 'neg'),\n",
    "         (\"I ain't feelin dandy today.\", 'neg'),\n",
    "         (\"I feel amazing!\", 'pos'),\n",
    "         ('Gary is a good friend of mine.', 'pos'),\n",
    "         (\"I can't believe I'm doing this.\", 'neg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the beer was good.', 'pos') pos\n",
      "('I do not enjoy my job', 'neg') neg\n",
      "(\"I ain't feelin dandy today.\", 'neg') neg\n",
      "('I feel amazing!', 'pos') pos\n",
      "('Gary is a good friend of mine.', 'pos') pos\n",
      "(\"I can't believe I'm doing this.\", 'neg') neg\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Create model and measure accuracy\n",
    "pipe.fit([x[0] for x in train], [x[1] for x in train]) \n",
    "pred_data = pipe.predict([x[0] for x in test])\n",
    "\n",
    "for (sample, pred) in zip(test, pred_data):\n",
    "    print(sample, pred)\n",
    "print(\"Accuracy:\", accuracy_score([x[1] for x in test], pred_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
